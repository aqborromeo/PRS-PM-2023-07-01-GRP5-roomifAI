{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/93/qvqgvzns6r359fg9jsgtqs700000gn/T/pip-req-build-z5whqi8b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/93/qvqgvzns6r359fg9jsgtqs700000gn/T/pip-req-build-z5whqi8b\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from clip==1.0) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: filelock in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torch->clip==1.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.5)\n",
      "Requirement already satisfied: requests in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/aqborromeo/Documents/MTECH-AI/scripts/NUS/NUS-ISS/Modules/PRS/.venv_blipcaption/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=ab618db7bb04111f7cecbd771d0a093131df674323fc0747b7c9fb9e879e5b28\n",
      "  Stored in directory: /private/var/folders/93/qvqgvzns6r359fg9jsgtqs700000gn/T/pip-ephem-wheel-cache-xo3kle6h/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U torch torchvision\n",
    "!pip install -U git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "def get_clip_score(image_path, text):\n",
    "# Load the pre-trained CLIP model and the image\n",
    "    model, preprocess = clip.load('ViT-B/32')\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text])\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_CLIP Score: 0.2901657223701477\n",
      "RoomifAI_V1_CLIP Score: 0.2811181843280792\n",
      "RoomifAI_V2_CLIP Score: 0.3061717748641968\n"
     ]
    }
   ],
   "source": [
    "# Prompt 1\n",
    "\n",
    "sd_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/sd_base_model/SD_Prompt1.jpg\"\n",
    "r1_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v1/Roomifai_v1_Prompt1.jpg\"\n",
    "r2_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v2/Roomifai_v2_Prompt1.jpg\"\n",
    "text = \"Scandinavian style bedroom with a queen size bed, wardrobe closet, photo frame hanging in the wall\"\n",
    "\n",
    "sd_score = get_clip_score(sd_image_path, text)\n",
    "print(f\"SD_CLIP Score: {sd_score}\")\n",
    "\n",
    "r1_score = get_clip_score(r1_image_path, text)\n",
    "print(f\"RoomifAI_V1_CLIP Score: {r1_score}\")\n",
    "\n",
    "r2_score = get_clip_score(r2_image_path, text)\n",
    "print(f\"RoomifAI_V2_CLIP Score: {r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_CLIP Score: 0.31389832496643066\n",
      "RoomifAI_V1_CLIP Score: 0.31286531686782837\n",
      "RoomifAI_V2_CLIP Score: 0.3326149880886078\n"
     ]
    }
   ],
   "source": [
    "# Prompt 2\n",
    "\n",
    "sd_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/sd_base_model/SD_Prompt2.jpg\"\n",
    "r1_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v1/Roomifai_v1_Prompt2.jpg\"\n",
    "r2_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v2/Roomifai_v2_Prompt2.jpg\"\n",
    "text = \"A bedroom with Korean wood king-size bed, side table with a lamp on top, a large window with view of sea, Korean wood nightstand\"\n",
    "\n",
    "sd_score = get_clip_score(sd_image_path, text)\n",
    "print(f\"SD_CLIP Score: {sd_score}\")\n",
    "\n",
    "r1_score = get_clip_score(r1_image_path, text)\n",
    "print(f\"RoomifAI_V1_CLIP Score: {r1_score}\")\n",
    "\n",
    "r2_score = get_clip_score(r2_image_path, text)\n",
    "print(f\"RoomifAI_V2_CLIP Score: {r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_CLIP Score: 0.2982986569404602\n",
      "RoomifAI_V1_CLIP Score: 0.29870519042015076\n",
      "RoomifAI_V2_CLIP Score: 0.30834031105041504\n"
     ]
    }
   ],
   "source": [
    "# Prompt 3\n",
    "\n",
    "sd_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/sd_base_model/SD_Prompt3.jpg\"\n",
    "r1_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v1/Roomifai_v1_Prompt3.jpg\"\n",
    "r2_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v2/Roomifai_v2_Prompt3.jpg\"\n",
    "text = \"Master bedroom with Korean rough cloth footstool, Minimalist drawer chest, corner cabinet, Modern  pendant lamp\"\n",
    "\n",
    "sd_score = get_clip_score(sd_image_path, text)\n",
    "print(f\"SD_CLIP Score: {sd_score}\")\n",
    "\n",
    "r1_score = get_clip_score(r1_image_path, text)\n",
    "print(f\"RoomifAI_V1_CLIP Score: {r1_score}\")\n",
    "\n",
    "r2_score = get_clip_score(r2_image_path, text)\n",
    "print(f\"RoomifAI_V2_CLIP Score: {r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_CLIP Score: 0.37600845098495483\n",
      "RoomifAI_V1_CLIP Score: 0.37309741973876953\n",
      "RoomifAI_V2_CLIP Score: 0.3833085298538208\n"
     ]
    }
   ],
   "source": [
    "# Prompt 4 ## Adjust the prompt\n",
    "\n",
    "sd_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/sd_base_model/SD_Prompt41.jpg\"\n",
    "r1_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v1/Roomifai_v1_Prompt4.jpg\"\n",
    "r2_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v2/Roomifai_v2_Prompt4.jpg\"\n",
    "text = \"a Nordic style child's room with a pink wall and a bed, rug\" \n",
    "\n",
    "sd_score = get_clip_score(sd_image_path, text)\n",
    "print(f\"SD_CLIP Score: {sd_score}\")\n",
    "\n",
    "r1_score = get_clip_score(r1_image_path, text)\n",
    "print(f\"RoomifAI_V1_CLIP Score: {r1_score}\")\n",
    "\n",
    "r2_score = get_clip_score(r2_image_path, text)\n",
    "print(f\"RoomifAI_V2_CLIP Score: {r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_CLIP Score: 0.32196131348609924\n",
      "RoomifAI_V1_CLIP Score: 0.30627191066741943\n",
      "RoomifAI_V2_CLIP Score: 0.33156487345695496\n"
     ]
    }
   ],
   "source": [
    "# Prompt 5\n",
    "\n",
    "sd_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/sd_base_model/SD_Prompt5.jpg\"\n",
    "r1_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v1/Roomifai_v1_Prompt5.jpg\"\n",
    "r2_image_path = \"/Users/aqborromeo/Downloads/roomifai/ai_generated_images/roomifai_v2/Roomifai_v2_Prompt5.jpg\"\n",
    "text =\"a bedroom with a bunk bed, a desk, Minimalist composite board desk, Modern  pendant lamp, Modern smooth leather lounge chair\"\n",
    "# text = \"a bedroom with a bunk bed and a desk, Mediterranean wood kids bed, Minimalist composite board desk, Modern  pendant lamp, Modern smooth leather lounge chair\"\n",
    "\n",
    "sd_score = get_clip_score(sd_image_path, text)\n",
    "print(f\"SD_CLIP Score: {sd_score}\")\n",
    "\n",
    "r1_score = get_clip_score(r1_image_path, text)\n",
    "print(f\"RoomifAI_V1_CLIP Score: {r1_score}\")\n",
    "\n",
    "r2_score = get_clip_score(r2_image_path, text)\n",
    "print(f\"RoomifAI_V2_CLIP Score: {r2_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_blipcaption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
