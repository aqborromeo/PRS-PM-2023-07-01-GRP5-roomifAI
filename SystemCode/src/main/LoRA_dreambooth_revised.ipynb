{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "# **Stable Diffusion - LoRA Dreambooth**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "_u3q60di584x"
      },
      "outputs": [],
      "source": [
        "# @title ## 1.1. Install Dependencies\n",
        "# @markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.  This will also install the required libraries.\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "\n",
        "# root_dir\n",
        "root_dir = \"./\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"model\")\n",
        "training_dir = os.path.join(root_dir, \"train\", \"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir, \"checkpoints\", \"pretrained_model\")\n",
        "vae_dir = os.path.join(root_dir, \"checkpoints\", \"vae\")\n",
        "config_dir = os.path.join(training_dir, \"config\")\n",
        "\n",
        "# repo_dir\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir = os.path.join(repo_dir, \"finetune\")\n",
        "\n",
        "# bitsandytes_main_py = \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py\"\n",
        "install_xformers = False \n",
        "mount_drive = False  # @param {type: \"boolean\"}\n",
        "verbose = False # @param {type: \"boolean\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3gob9_OwTlwh"
      },
      "source": [
        "# II. Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "8VT6NLv-2u6q"
      },
      "outputs": [],
      "source": [
        "# @title ## 2.1. Download Available Model\n",
        "import os\n",
        "\n",
        "# Get model here: https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/stable_diffusion_1_5-pruned.safetensors\n",
        "\n",
        "models = {\n",
        "    \"Stable-Diffusion-v1-5\": \"./checkpoints/pretrained_model/v1-5-pruned-emaonly.safetensors\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "qrY4KtfL6Dqp"
      },
      "outputs": [],
      "source": [
        "# @title ## 2.3. Download Available VAE (Optional)\n",
        "import os\n",
        "\n",
        "# Download compatible SD v1.5 VAE from \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\" \n",
        "\n",
        "vaes = {\n",
        "    \"stablediffusion.vae.pt\": \"./checkpoints/vae-ft-mse-840000-ema-pruned.ckpt\",\n",
        "}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T-0qKyEgTchp"
      },
      "source": [
        "# III. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get data from training folder\n",
        "import os\n",
        "\n",
        "train_data_dir = \"./train/LoRA/train_data\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yHNbl3O_NSS0"
      },
      "source": [
        "# Model Training \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "H_Q23fUEJhnC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project Name:  roomifai_living\n",
            "Pretrained Model Path:  ./checkpoints/pretrained_model/v1-5-pruned-emaonly.safetensors\n",
            "VAE path:  ./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt\n",
            "Output path:  ./train/LoRA/output\n"
          ]
        }
      ],
      "source": [
        "# @title ## Model Config\n",
        "# Change the key to:\n",
        "# - living\n",
        "# - bedroom\n",
        "# - dining\n",
        "key = \"living\"\n",
        "project_name = \"roomifai_\" + key\n",
        "if not project_name:\n",
        "    project_name = \"last\"\n",
        "pretrained_model_name_or_path = \"./checkpoints/pretrained_model/v1-5-pruned-emaonly.safetensors\"\n",
        "vae = \"./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt\"\n",
        "output_dir = \"./train/LoRA/output\"\n",
        "output_to_drive = True\n",
        "\n",
        "sample_dir = os.path.join(output_dir, \"sample\")\n",
        "for dir in [output_dir, sample_dir]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(\"Project Name: \", project_name)\n",
        "print(\n",
        "    \"Pretrained Model Path: \", pretrained_model_name_or_path\n",
        ") if pretrained_model_name_or_path else print(\"No pretrained model specified.\")\n",
        "print(\"VAE path: \", vae) if vae else print(\"No VAE path specified.\")\n",
        "print(\"Output path: \", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "G5u_DhFeyJ6R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[datasets]]\n",
            "resolution = 512\n",
            "min_bucket_reso = 256\n",
            "max_bucket_reso = 1024\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0\n",
            "caption_dropout_every_n_epochs = 0\n",
            "flip_aug = true\n",
            "color_aug = false\n",
            "[[datasets.subsets]]\n",
            "image_dir = \"./train/LoRA/train_data\"\n",
            "class_tokens = \"roomifai\"\n",
            "num_repeats = 1\n",
            "\n",
            "[[datasets.subsets]]\n",
            "image_dir = \"./train/LoRA/train_data\\\\10_living\"\n",
            "class_tokens = \"living\"\n",
            "num_repeats = 10\n",
            "\n",
            "\n",
            "[general]\n",
            "enable_bucket = true\n",
            "caption_extension = \".txt\"\n",
            "shuffle_caption = true\n",
            "keep_tokens = 0\n",
            "bucket_reso_steps = 64\n",
            "bucket_no_upscale = false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5.2. Dataset Config\n",
        "import os\n",
        "import toml\n",
        "import glob\n",
        "\n",
        "dataset_repeats = 1\n",
        "# `activation_word` is not used in training if you train with captions, but it is still printed to metadata.\n",
        "activation_word = \"roomifai\"\n",
        "caption_extension = \".txt\"\n",
        "resolution = 512\n",
        "flip_aug = True\n",
        "keep_tokens = 0\n",
        "\n",
        "# Place the images and captions under the training directory\n",
        "# Name it as {number of steps}_{class}\n",
        "# e.g. 10_living\n",
        "\n",
        "def parse_folder_name(folder_name, default_num_repeats, default_class_token):\n",
        "    folder_name_parts = folder_name.split(\"_\")\n",
        "\n",
        "    if len(folder_name_parts) == 2:\n",
        "        if folder_name_parts[0].isdigit():\n",
        "            num_repeats = int(folder_name_parts[0])\n",
        "            class_token = folder_name_parts[1].replace(\"_\", \" \")\n",
        "        else:\n",
        "            num_repeats = default_num_repeats\n",
        "            class_token = default_class_token\n",
        "    else:\n",
        "        num_repeats = default_num_repeats\n",
        "        class_token = default_class_token\n",
        "\n",
        "    return num_repeats, class_token\n",
        "\n",
        "def find_image_files(path):\n",
        "    supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "    return [file for file in glob.glob(path + '/**/*', recursive=True) if file.lower().endswith(supported_extensions)]\n",
        "\n",
        "def process_data_dir(data_dir, default_num_repeats, default_class_token, is_reg=False):\n",
        "    subsets = []\n",
        "\n",
        "    images = find_image_files(data_dir)\n",
        "    if images:\n",
        "        subsets.append({\n",
        "            \"image_dir\": data_dir,\n",
        "            \"class_tokens\": default_class_token,\n",
        "            \"num_repeats\": default_num_repeats,\n",
        "            **({\"is_reg\": is_reg} if is_reg else {}),\n",
        "        })\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for folder in dirs:\n",
        "            folder_path = os.path.join(root, folder)\n",
        "            images = find_image_files(folder_path)\n",
        "\n",
        "            if images:\n",
        "                num_repeats, class_token = parse_folder_name(folder, default_num_repeats, default_class_token)\n",
        "\n",
        "                subset = {\n",
        "                    \"image_dir\": folder_path,\n",
        "                    \"class_tokens\": class_token,\n",
        "                    \"num_repeats\": num_repeats,\n",
        "                }\n",
        "\n",
        "                if is_reg:\n",
        "                    subset[\"is_reg\"] = True\n",
        "\n",
        "                subsets.append(subset)\n",
        "\n",
        "    return subsets\n",
        "\n",
        "\n",
        "train_subsets = process_data_dir(train_data_dir, dataset_repeats, activation_word)\n",
        "\n",
        "subsets = train_subsets\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "            \"caption_dropout_rate\": 0,\n",
        "            \"caption_tag_dropout_rate\": 0,\n",
        "            \"caption_dropout_every_n_epochs\": 0,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": subsets,\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "iD5Ecamp4rVW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- LoRA Config:\n",
            "  - Loading network module: networks.lora\n",
            "  - networks.lora linear_dim set to: 32\n",
            "  - networks.lora linear_alpha set to: 16\n",
            "  - No LoRA weight loaded.\n",
            "- Optimizer Config:\n",
            "  - Additional network category: LoRA\n",
            "  - Using AdamW8bit as Optimizer\n",
            "  - Train UNet and Text Encoder\n",
            "    - UNet learning rate: 0.0001\n",
            "    - Text encoder learning rate: 5e-05\n",
            "  - Learning rate warmup steps: 0\n",
            "  - Learning rate Scheduler: constant\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5.3. LoRA and Optimizer Config\n",
        "\n",
        "# @markdown ### LoRA Config:\n",
        "network_category = \"LoRA\"\n",
        "conv_dim = 32\n",
        "conv_alpha = 16\n",
        "network_dim = 32\n",
        "network_alpha = 16\n",
        "# For resume training.\n",
        "network_weight = \"\"\n",
        "network_module = \"networks.lora\"\n",
        "network_args = \"\" if network_category == \"LoRA\" else [\n",
        "    f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\",\n",
        "    ]\n",
        "\n",
        "# @markdown ### <br>Optimizer Config:\n",
        "# `NEW` Gamma for reducing the weight of high-loss timesteps. Lower numbers have a stronger effect. The paper recommends 5. Read the paper [here](https://arxiv.org/abs/2303.09556).\n",
        "min_snr_gamma = -1\n",
        "optimizer_type = \"AdamW8bit\"\n",
        "optimizer_args = \"\"\n",
        "train_unet = True \n",
        "unet_lr = 1e-4\n",
        "train_text_encoder = True \n",
        "text_encoder_lr = 5e-5\n",
        "lr_scheduler = \"constant\"\n",
        "lr_warmup_steps = 0\n",
        "lr_scheduler_num_cycles = 0\n",
        "lr_scheduler_power = 0\n",
        "\n",
        "if network_category == \"LoHa\":\n",
        "  network_args.append(\"algo=loha\")\n",
        "elif network_category == \"LoCon_Lycoris\":\n",
        "  network_args.append(\"algo=lora\")\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(f\"  - Min-SNR Weighting: {min_snr_gamma}\") if not min_snr_gamma == -1 else \"\"\n",
        "print(f\"  - Loading network module: {network_module}\")\n",
        "if not network_category == \"LoRA\":\n",
        "  print(f\"  - network args: {network_args}\")\n",
        "print(f\"  - {network_module} linear_dim set to: {network_dim}\")\n",
        "print(f\"  - {network_module} linear_alpha set to: {network_alpha}\")\n",
        "if not network_category == \"LoRA\":\n",
        "  print(f\"  - {network_module} conv_dim set to: {conv_dim}\")\n",
        "  print(f\"  - {network_module} conv_alpha set to: {conv_alpha}\")\n",
        "\n",
        "if not network_weight:\n",
        "    print(\"  - No LoRA weight loaded.\")\n",
        "else:\n",
        "    if os.path.exists(network_weight):\n",
        "        print(f\"  - Loading LoRA weight: {network_weight}\")\n",
        "    else:\n",
        "        print(f\"  - {network_weight} does not exist.\")\n",
        "        network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"  - Additional network category: {network_category}\")\n",
        "print(f\"  - Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "    print(f\"  - Optimizer Args: {optimizer_args}\")\n",
        "if train_unet and train_text_encoder:\n",
        "    print(\"  - Train UNet and Text Encoder\")\n",
        "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
        "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
        "if train_unet and not train_text_encoder:\n",
        "    print(\"  - Train UNet only\")\n",
        "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
        "if train_text_encoder and not train_unet:\n",
        "    print(\"  - Train Text Encoder only\")\n",
        "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
        "print(f\"  - Learning rate warmup steps: {lr_warmup_steps}\")\n",
        "print(f\"  - Learning rate Scheduler: {lr_scheduler}\")\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "    print(f\"  - lr_scheduler_num_cycles: {lr_scheduler_num_cycles}\")\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "    print(f\"  - lr_scheduler_power: {lr_scheduler_power}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "-Z4w3lfFKLjr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[model_arguments]\n",
            "v2 = false\n",
            "v_parameterization = false\n",
            "pretrained_model_name_or_path = \"./checkpoints/pretrained_model/v1-5-pruned-emaonly.safetensors\"\n",
            "vae = \"./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt\"\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "unet_lr = 0.0001\n",
            "text_encoder_lr = 5e-5\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 32\n",
            "network_alpha = 16\n",
            "network_train_unet_only = false\n",
            "network_train_text_encoder_only = false\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"AdamW8bit\"\n",
            "learning_rate = 0.0001\n",
            "max_grad_norm = 1.0\n",
            "lr_scheduler = \"constant\"\n",
            "lr_warmup_steps = 0\n",
            "\n",
            "[dataset_arguments]\n",
            "cache_latents = true\n",
            "debug_dataset = false\n",
            "vae_batch_size = 4\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"./train/LoRA/output\"\n",
            "output_name = \"roomifai_living\"\n",
            "save_precision = \"bf16\"\n",
            "save_every_n_epochs = 1\n",
            "train_batch_size = 6\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "xformers = true\n",
            "max_train_epochs = 4\n",
            "max_data_loader_n_workers = 8\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = false\n",
            "gradient_accumulation_steps = 1\n",
            "mixed_precision = \"bf16\"\n",
            "clip_skip = 2\n",
            "project_dir = \"./train/LoRA/logs\"\n",
            "log_prefix = \"roomifai_living\"\n",
            "lowram = true\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 1\n",
            "sample_sampler = \"euler_a\"\n",
            "\n",
            "[dreambooth_arguments]\n",
            "prior_loss_weight = 1.0\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## 5.4. Training Config\n",
        "\n",
        "import toml\n",
        "import os\n",
        "\n",
        "lowram = True\n",
        "enable_sample_prompt = True\n",
        "sampler = \"euler_a\"\n",
        "noise_offset = 0.0\n",
        "num_epochs = 4\n",
        "vae_batch_size = 4\n",
        "train_batch_size = 6\n",
        "mixed_precision = \"bf16\"\n",
        "save_precision = \"bf16\"\n",
        "save_n_epochs_type = \"save_every_n_epochs\"  # @param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"]\n",
        "save_n_epochs_type_value = 1\n",
        "save_model_as = \"safetensors\"\n",
        "max_token_length = 225\n",
        "clip_skip = 2\n",
        "gradient_checkpointing = False\n",
        "gradient_accumulation_steps = 1\n",
        "seed = -1\n",
        "logging_dir = \"./train/LoRA/logs\"\n",
        "prior_loss_weight = 1.0\n",
        "\n",
        "# os.chdir(repo_dir)\n",
        "\n",
        "sample_str = f\"\"\"\n",
        "  masterpiece, best quality, living room with minimalist modern wooden furniture \\\\\n",
        "  --w 512 \\\n",
        "  --h 512 \\\n",
        "  --l 7 \\\n",
        "  --s 28    \n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": False,\n",
        "        \"v_parameterization\": False,\n",
        "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"min_snr_gamma\": min_snr_gamma if not min_snr_gamma == -1 else None,\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "        \"vae_batch_size\": vae_batch_size,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": True,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip,\n",
        "        \"project_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\": {\n",
        "        \"sample_every_n_steps\": None,\n",
        "        \"sample_every_n_epochs\": 1 if enable_sample_prompt else 999999,\n",
        "        \"sample_sampler\": sampler,\n",
        "    },\n",
        "    \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "write_file(config_path, config_str)\n",
        "write_file(prompt_path, sample_str)\n",
        "    \n",
        "print(config_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "p_SHtbFwHVl1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running:  accelerate launch --config_file=\"./model\\accelerate_config/config.yaml\" --num_cpu_threads_per_process=1  train_network.py --sample_prompts=\"./train\\LoRA\\config\\sample_prompt.txt\" --dataset_config=\"./train\\LoRA\\config\\dataset_config.toml\" --config_file=\"./train\\LoRA\\config\\config_file.toml\" \n",
            "Loading settings from ./train\\LoRA\\config\\config_file.toml...\n",
            "./train\\LoRA\\config\\config_file\n",
            "prepare tokenizer\n",
            "update token length: 225\n",
            "Loading dataset config from train\\LoRA\\config\\dataset_config.toml\n",
            "prepare images.\n",
            "found directory ./train/LoRA/train_data contains 0 image files\n",
            "ignore subset with image_dir='./train/LoRA/train_data': no images found / 画像が見つからないためサブセットを無視します\n",
            "found directory ./train/LoRA/train_data\\10_living contains 160 image files\n",
            "1600 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 6\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"./train/LoRA/train_data\\10_living\"\n",
            "    image_count: 160\n",
            "    num_repeats: 10\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0\n",
            "    caption_prefix: None\n",
            "    caption_suffix: None\n",
            "    color_aug: False\n",
            "    flip_aug: True\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: living\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (512, 512), count: 1600\n",
            "mean ar error (without repeats): 0.0\n",
            "preparing accelerator\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint: ./checkpoints/pretrained_model/v1-5-pruned-emaonly.safetensors\n",
            "UNet2DConditionModel: 64, 8, 768, False, False\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: ./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt\n",
            "exception occurs in loading vae: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like ./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt is not the path to a directory containing a file named diffusion_pytorch_model.bin or \n",
            "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.\n",
            "retry with subfolder='vae'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/160 [00:00<?, ?it/s]\n",
            "100%|██████████| 160/160 [00:00<00:00, 1713.04it/s]\n",
            "c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\safetensors\\torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 857, in _get_model_file\n",
            "    model_file = hf_hub_download(\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 158, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt'. Use `repo_type` argument if needed.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\library\\model_util.py\", line 1266, in load_vae\n",
            "    vae = AutoencoderKL.from_pretrained(vae_id, subfolder=None, torch_dtype=dtype)\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 527, in from_pretrained\n",
            "    model_file = _get_model_file(\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 894, in _get_model_file\n",
            "    raise EnvironmentError(\n",
            "OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like ./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt is not the path to a directory containing a file named diffusion_pytorch_model.bin or \n",
            "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 857, in _get_model_file\n",
            "    model_file = hf_hub_download(\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 158, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt'. Use `repo_type` argument if needed.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\train_network.py\", line 1006, in <module>\n",
            "    trainer.train(args)\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\train_network.py\", line 224, in train\n",
            "    model_version, text_encoder, vae, unet = self.load_target_model(args, weight_dtype, accelerator)\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\train_network.py\", line 101, in load_target_model\n",
            "    text_encoder, vae, unet, _ = train_util.load_target_model(args, weight_dtype, accelerator)\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\library\\train_util.py\", line 3865, in load_target_model\n",
            "    text_encoder, vae, unet, load_stable_diffusion_format = _load_target_model(\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\library\\train_util.py\", line 3842, in _load_target_model\n",
            "    vae = model_util.load_vae(args.vae, weight_dtype)\n",
            "  File \"c:\\Users\\JackChua\\Projects\\NUS\\SEM02\\PRS-PM-2023-07-01-GRP5-roomifAI\\SystemCode\\src\\main\\library\\model_util.py\", line 1270, in load_vae\n",
            "    vae = AutoencoderKL.from_pretrained(vae_id, subfolder=\"vae\", torch_dtype=dtype)\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 527, in from_pretrained\n",
            "    model_file = _get_model_file(\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 894, in _get_model_file\n",
            "    raise EnvironmentError(\n",
            "OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like ./checkpoints/checkpoints/vae/vae-ft-mse-840000-ema-pruned.ckpt is not the path to a directory containing a file named diffusion_pytorch_model.bin or \n",
            "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"C:\\Users\\JackChua\\miniconda3\\envs\\room\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 923, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"c:\\users\\jackchua\\miniconda3\\envs\\room\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 579, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['c:\\\\users\\\\jackchua\\\\miniconda3\\\\envs\\\\room\\\\python.exe', 'train_network.py', '--sample_prompts=./train\\\\LoRA\\\\config\\\\sample_prompt.txt', '--dataset_config=./train\\\\LoRA\\\\config\\\\dataset_config.toml', '--config_file=./train\\\\LoRA\\\\config\\\\config_file.toml']' returned non-zero exit status 1.\n"
          ]
        }
      ],
      "source": [
        "#@title ## 5.5. Start Training\n",
        "sample_prompt = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "config_file = os.path.join(config_dir, \"config_file.toml\")\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "accelerate_conf = {\n",
        "    \"config_file\" : accelerate_config,\n",
        "    \"num_cpu_threads_per_process\" : 1,\n",
        "}\n",
        "\n",
        "train_conf = {\n",
        "    \"sample_prompts\" : sample_prompt,\n",
        "    \"dataset_config\" : dataset_config,\n",
        "    \"config_file\" : config_file\n",
        "}\n",
        "\n",
        "def train(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    return args\n",
        "\n",
        "accelerate_args = train(accelerate_conf)\n",
        "train_args = train(train_conf)\n",
        "final_args = f\"accelerate launch {accelerate_args} train_network.py {train_args}\"\n",
        "\n",
        "print(\"Running: \", final_args)\n",
        "\n",
        "# os.chdir(repo_dir)\n",
        "!{final_args}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "Rt7CKCog_4tm"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: ''",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     print_metadata(file_path)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[14], line 48\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(file_path, verbose)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(file_path, verbose: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     weight_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_weight_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     51\u001b[0m         lora_weights \u001b[38;5;241m=\u001b[39m extract_lora_weights(weight_data)\n",
            "Cell \u001b[1;32mIn[14], line 20\u001b[0m, in \u001b[0;36mload_weight_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_file(file_path)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JackChua\\miniconda3\\envs\\room\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
            "File \u001b[1;32mc:\\Users\\JackChua\\miniconda3\\envs\\room\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[1;32mc:\\Users\\JackChua\\miniconda3\\envs\\room\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ],
      "source": [
        "# @title ## 6.2. Interrogating LoRA Weights\n",
        "# @markdown Now you can check if your LoRA trained properly.\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from safetensors.torch import load_file\n",
        "from safetensors.torch import safe_open\n",
        "\n",
        "# @markdown If you used `clip_skip = 2` during training, the values of `lora_te_text_model_encoder_layers_11_*` will be `0.0`, this is normal. These layers are not trained at this value of `Clip Skip`.\n",
        "network_weight = \"\"\n",
        "verbose = False\n",
        "\n",
        "def is_safetensors(path):\n",
        "    return os.path.splitext(path)[1].lower() == \".safetensors\"\n",
        "\n",
        "def load_weight_data(file_path):\n",
        "    if is_safetensors(file_path):\n",
        "        return load_file(file_path)\n",
        "    else:\n",
        "        return torch.load(file_path, map_location=\"cuda\")\n",
        "\n",
        "def extract_lora_weights(weight_data):\n",
        "    lora_weights = [\n",
        "        (key, weight_data[key])\n",
        "        for key in weight_data.keys()\n",
        "        if \"lora_up\" in key or \"lora_down\" in key\n",
        "    ]\n",
        "    return lora_weights\n",
        "\n",
        "def print_lora_weight_stats(lora_weights):\n",
        "    print(f\"Number of LoRA modules: {len(lora_weights)}\")\n",
        "\n",
        "    for key, value in lora_weights:\n",
        "        value = value.to(torch.float32)\n",
        "        print(f\"{key}, {torch.mean(torch.abs(value))}, {torch.min(torch.abs(value))}\")\n",
        "\n",
        "def print_metadata(file_path):\n",
        "    if is_safetensors(file_path):\n",
        "        with safe_open(file_path, framework=\"pt\") as f:\n",
        "            metadata = f.metadata()\n",
        "        if metadata is not None:\n",
        "            print(f\"\\nLoad metadata for: {file_path}\")\n",
        "            print(json.dumps(metadata, indent=4))\n",
        "    else:\n",
        "        print(\"No metadata saved, your model is not in safetensors format\")\n",
        "\n",
        "def main(file_path, verbose: bool):\n",
        "    weight_data = load_weight_data(file_path)\n",
        "\n",
        "    if verbose:\n",
        "        lora_weights = extract_lora_weights(weight_data)\n",
        "        print_lora_weight_stats(lora_weights)\n",
        "\n",
        "    print_metadata(file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(network_weight, verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FKBrTDPrcNjP"
      },
      "outputs": [],
      "source": [
        "# @title ## 6.3. Inference\n",
        "%store -r\n",
        "\n",
        "# @markdown ### LoRA Config\n",
        "# @markdown Currently, `LoHa` and `LoCon_Lycoris` are not supported. Please run `Portable Web UI` instead\n",
        "network_weight = \"\"\n",
        "network_mul = 0.7  # @param {type:\"slider\", min:-1, max:2, step:0.05}\n",
        "network_module = \"networks.lora\"\n",
        "network_args = \"\"\n",
        "\n",
        "# @markdown ### <br> General Config\n",
        "v2 = False\n",
        "v_parameterization = False\n",
        "prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"  # @param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"  # @param {type: \"string\"}\n",
        "model = \"/content/pretrained_model/AnyLoRA.safetensors\"  # @param {type: \"string\"}\n",
        "vae = \"\"  # @param {type: \"string\"}\n",
        "outdir = \"/content/tmp\"  # @param {type: \"string\"}\n",
        "scale = 7  # @param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 28  # @param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\"  # @param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512  # @param {type: \"integer\"}\n",
        "height = 768  # @param {type: \"integer\"}\n",
        "images_per_prompt = 4  # @param {type: \"integer\"}\n",
        "batch_size = 4  # @param {type: \"integer\"}\n",
        "clip_skip = 2  # @param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1  # @param {type: \"integer\"}\n",
        "\n",
        "final_prompt = f\"{prompt} --n {negative}\"\n",
        "\n",
        "config = {\n",
        "    \"v2\": v2,\n",
        "    \"v_parameterization\": v_parameterization,\n",
        "    \"network_module\": network_module,\n",
        "    \"network_weight\": network_weight,\n",
        "    \"network_mul\": float(network_mul),\n",
        "    \"network_args\": eval(network_args) if network_args else None,\n",
        "    \"ckpt\": model,\n",
        "    \"outdir\": outdir,\n",
        "    \"xformers\": True,\n",
        "    \"vae\": vae if vae else None,\n",
        "    \"fp16\": True,\n",
        "    \"W\": width,\n",
        "    \"H\": height,\n",
        "    \"seed\": seed if seed > 0 else None,\n",
        "    \"scale\": scale,\n",
        "    \"sampler\": sampler,\n",
        "    \"steps\": steps,\n",
        "    \"max_embeddings_multiples\": 3,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"images_per_prompt\": images_per_prompt,\n",
        "    \"clip_skip\": clip_skip if not v2 else None,\n",
        "    \"prompt\": final_prompt,\n",
        "}\n",
        "\n",
        "args = \"\"\n",
        "for k, v in config.items():\n",
        "    if k.startswith(\"_\"):\n",
        "        args += f'\"{v}\" '\n",
        "    elif isinstance(v, str):\n",
        "        args += f'--{k}=\"{v}\" '\n",
        "    elif isinstance(v, bool) and v:\n",
        "        args += f\"--{k} \"\n",
        "    elif isinstance(v, float) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "    elif isinstance(v, int) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "\n",
        "final_args = f\"python gen_img_diffusers.py {args}\"\n",
        "\n",
        "# os.chdir(repo_dir)\n",
        "!{final_args}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.17 ('room')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "33e8c653b61fcd30db27bb9bf153aa9abf8c95d57f0a0708ce05bf3f3ed84366"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
